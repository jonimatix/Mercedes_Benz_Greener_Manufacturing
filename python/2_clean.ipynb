{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_all_cleaned = pd.read_csv(\"../../data/Mercedes_Benz_Greener_Manufacturing/data/dt_all_transformed.csv\")\n",
    "cols_cat = pickle.load(open(\"../../data/Mercedes_Benz_Greener_Manufacturing/data/cols_cat.pkl\", \"rb\"))\n",
    "IDs_train = pickle.load(open(\"../../data/Mercedes_Benz_Greener_Manufacturing/data/IDs_train.pkl\", \"rb\"))\n",
    "IDs_test = pickle.load(open(\"../../data/Mercedes_Benz_Greener_Manufacturing/data/IDs_test.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_train_raw = dt_all_cleaned.loc[dt_all_cleaned[\"ID\"].isin(IDs_train)]\n",
    "dt_test_raw = dt_all_cleaned.loc[dt_all_cleaned[\"ID\"].isin(IDs_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4209, 378) (4209, 378)\n"
     ]
    }
   ],
   "source": [
    "print(dt_train_raw.shape, dt_test_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Duplicated cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Drop dup cols in dt_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# duplicated cols in dt_all\n",
    "cols_dup_all_toDrop = dt_all_cleaned.T.duplicated()[dt_all_cleaned.T.duplicated() == True].index.values\n",
    "dt_all_cleaned = dt_all_cleaned.drop(cols_dup_all_toDrop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 342)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_all_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Rename the remaining dup cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# duplicated cols in dt_train\n",
    "cols_dup_train = dt_train_raw.T.duplicated(keep = False)[dt_train_raw.T.duplicated(keep = False) == True].index.values\n",
    "# duplicated cols in dt_test\n",
    "cols_dup_test = dt_test_raw.T.duplicated(keep = False)[dt_test_raw.T.duplicated(keep = False) == True].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change col names for cols_dup_train and cols_dup_test\n",
    "dict_dup_train = {x: \"dup_train_\" + x for x in list(cols_dup_train)}\n",
    "dt_all_cleaned = dt_all_cleaned.rename(columns = dict_dup_train)\n",
    "dict_dup_test = {x: \"dup_test_\" + x for x in list(cols_dup_test[cols_dup_test != \"y\"])}\n",
    "dt_all_cleaned = dt_all_cleaned.rename(columns = dict_dup_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 342)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_all_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Duplicated rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Rename them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cols_cat\n",
    "cols_cat = dt_all_cleaned.select_dtypes(include = ['object']).columns.values\n",
    "# cols_int\n",
    "cols_int = dt_all_cleaned.drop(\"ID\", axis = 1).select_dtypes(include = ['int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 342)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_all_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Mean y of duplicated row(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Remove single values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# single value cols in dt_train\n",
    "cols_single_train = []\n",
    "for col in dt_all_cleaned.drop([\"y\"], axis = 1).columns.values:\n",
    "    len_unique = len(np.unique(dt_all_cleaned.loc[dt_all_cleaned[\"ID\"].isin(IDs_train)][col].values))\n",
    "    if len_unique == 1:\n",
    "        cols_single_train.append(col)\n",
    "# single value cols in dt_test\n",
    "cols_single_test = []\n",
    "for col in dt_all_cleaned.drop([\"y\"], axis = 1).columns.values:\n",
    "    len_unique = len(np.unique(dt_all_cleaned.loc[dt_all_cleaned[\"ID\"].isin(IDs_test)][col].values))\n",
    "    if len_unique == 1:\n",
    "        cols_single_test.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change col names for cols_single_train and cols_single_test\n",
    "dict_single_train = {x: \"single_train_\" + x for x in cols_single_train}\n",
    "dt_all_cleaned = dt_all_cleaned.rename(columns = dict_single_train)\n",
    "dict_single_test = {x: \"single_test_\" + x for x in cols_single_test}\n",
    "dt_all_cleaned = dt_all_cleaned.rename(columns = dict_single_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 342)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_all_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Remove complimentary cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_int = dt_all_cleaned.drop(\"ID\", axis = 1).select_dtypes(include = [\"int64\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def removeCompCols(dt, cols):\n",
    "    seen = []\n",
    "    col2s = []\n",
    "    nrow = dt.shape[0]\n",
    "    for col1 in cols_int:\n",
    "        for col2 in cols_int:\n",
    "            compliment = sum(dt[col1].values + dt[col2].values)\n",
    "            same = np.sum(dt[col1] == dt[col2])\n",
    "            if (compliment == nrow) & (same == 0):\n",
    "                seen.append((col1, col2))\n",
    "                if (col2, col1) not in seen:\n",
    "                    col2s.append(col2)\n",
    "                    print(col1, col2)\n",
    "    return col2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X128 X130\n",
      "X156 X157\n",
      "X204 X205\n",
      "dup_train_X232 dup_test_X263\n"
     ]
    }
   ],
   "source": [
    "cols_comp = removeCompCols(dt_all_cleaned, cols_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_all_cleaned = dt_all_cleaned.drop(cols_comp, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 338)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_all_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Save cols_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dt_all_cleaned\n",
    "dt_all_cleaned.to_csv(\"../../data/Mercedes_Benz_Greener_Manufacturing/data/dt_all_cleaned.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
