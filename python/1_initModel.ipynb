{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, train_test_split\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import linear_model, decomposition\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "dt_train_raw = pd.read_csv(\"../../data/Mercedes_Benz_Greener_Manufacturing/raw/train.csv\")\n",
    "dt_test_raw = pd.read_csv(\"../../data/Mercedes_Benz_Greener_Manufacturing/raw/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4209, 378) (4209, 377)\n"
     ]
    }
   ],
   "source": [
    "print(dt_train_raw.shape, dt_test_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# marke train and test set\n",
    "dt_train_raw.loc[:, \"IsTrainTest\"] = pd.Series(\"train\", index = dt_train_raw.index)\n",
    "dt_test_raw.loc[:, \"IsTrainTest\"] = pd.Series(\"test\", index = dt_test_raw.index)\n",
    "\n",
    "# change test index and add y\n",
    "dt_test_raw.index = dt_test_raw.index + max(dt_train_raw.index) + 1\n",
    "dt_test_raw.loc[:,\"y\"] = pd.Series([0.0] * dt_test_raw.shape[0], index = dt_test_raw.index)\n",
    "\n",
    "# concat\n",
    "dt_test_raw = dt_test_raw[dt_train_raw.columns.values]\n",
    "dt_all_raw = pd.concat([dt_train_raw, dt_test_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 379)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_all_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Duplicated cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Drop dup cols in dt_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# duplicated cols in dt_all\n",
    "cols_dup_all_toDrop = dt_all_raw.T.duplicated()[dt_all_raw.T.duplicated() == True].index.values\n",
    "dt_all_raw = dt_all_raw.drop(cols_dup_all_toDrop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 343)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_all_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Rename the remaining dup cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# duplicated cols in dt_train\n",
    "cols_dup_train = dt_train_raw.T.duplicated(keep = False)[dt_train_raw.T.duplicated(keep = False) == True].index.values\n",
    "# duplicated cols in dt_test\n",
    "cols_dup_test = dt_test_raw.T.duplicated(keep = False)[dt_test_raw.T.duplicated(keep = False) == True].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change col names for cols_dup_train and cols_dup_test\n",
    "dict_dup_train = {x: \"dup_train_\" + x for x in list(cols_dup_train)}\n",
    "dt_all_raw = dt_all_raw.rename(columns = dict_dup_train)\n",
    "dict_dup_test = {x: \"dup_test_\" + x for x in list(cols_dup_test[cols_dup_test != \"y\"])}\n",
    "dt_all_raw = dt_all_raw.rename(columns = dict_dup_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 343)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_all_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cols_cat\n",
    "cols_cat = dt_all_raw.drop(\"IsTrainTest\", axis = 1).select_dtypes(include = ['object']).columns.values\n",
    "# cols_int\n",
    "cols_int = dt_all_raw.drop(\"ID\", axis = 1).select_dtypes(include = ['int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_all_raw.loc[:, \"IsDupRow_All\"] = dt_all_raw.drop([\"ID\", \"y\"], axis = 1).duplicated(keep = False).astype(\"int64\")\n",
    "dt_all_raw.loc[:, \"IsDupRow_Cat\"] = dt_all_raw.drop([\"ID\", \"y\"], axis = 1)[cols_cat].duplicated(keep = False).astype(\"int64\")\n",
    "dt_all_raw.loc[:, \"IsDupRow_Int\"] = dt_all_raw.drop([\"ID\", \"y\"], axis = 1)[cols_int].duplicated(keep = False).astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 346)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_all_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Remove single values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# single value cols in dt_train\n",
    "cols_single_train = []\n",
    "for col in dt_all_raw.loc[dt_all_raw[\"IsTrainTest\"] == \"train\"].drop([\"y\", \"IsTrainTest\"], axis = 1).columns.values:\n",
    "    len_unique = len(np.unique(dt_all_raw.loc[dt_all_raw[\"IsTrainTest\"] == \"train\"][col].values))\n",
    "    if len_unique == 1:\n",
    "        cols_single_train.append(col)\n",
    "# single value cols in dt_test\n",
    "cols_single_test = []\n",
    "for col in dt_all_raw.loc[dt_all_raw[\"IsTrainTest\"] == \"test\"].drop([\"y\", \"IsTrainTest\"], axis = 1).columns.values:\n",
    "    len_unique = len(np.unique(dt_all_raw.loc[dt_all_raw[\"IsTrainTest\"] == \"test\"][col].values))\n",
    "    if len_unique == 1:\n",
    "        cols_single_test.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change col names for cols_single_train and cols_single_test\n",
    "dict_single_train = {x: \"single_train_\" + x for x in cols_single_train}\n",
    "dt_all_raw = dt_all_raw.rename(columns = dict_single_train)\n",
    "dict_single_test = {x: \"single_test_\" + x for x in cols_single_test}\n",
    "dt_all_raw = dt_all_raw.rename(columns = dict_single_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 346)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_all_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Remove complimentary cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_int = dt_all_raw.drop(\"ID\", axis = 1).select_dtypes(include = ['int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def removeCompCols(dt, cols):\n",
    "    seen = []\n",
    "    col2s = []\n",
    "    nrow = dt.shape[0]\n",
    "    for col1 in cols_int:\n",
    "        for col2 in cols_int:\n",
    "            compliment = sum(dt[col1].values + dt[col2].values)\n",
    "            same = np.sum(dt[col1] == dt[col2])\n",
    "            if (compliment == nrow) & (same == 0):\n",
    "                seen.append((col1, col2))\n",
    "                if (col2, col1) not in seen:\n",
    "                    col2s.append(col2)\n",
    "                    print(col1, col2)\n",
    "    return col2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X128 X130\n",
      "X156 X157\n",
      "X204 X205\n",
      "dup_train_X232 dup_test_X263\n"
     ]
    }
   ],
   "source": [
    "cols_comp = removeCompCols(dt_all_raw, cols_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_all_raw = dt_all_raw.drop(cols_comp, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 342)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_all_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Save cols_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_raw\n",
    "cols_raw = dt_all_raw.drop([\"ID\", \"y\", \"IsTrainTest\", \"IsDupRow_All\", \"IsDupRow_Cat\", \"IsDupRow_Int\"\n",
    "                            , \"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"], axis = 1).columns.values\n",
    "pickle.dump(cols_raw, open( \"../../data/Mercedes_Benz_Greener_Manufacturing/data/cols_raw.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Encode cat cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cols_cat\n",
    "cols_cat = dt_all_raw.drop(\"IsTrainTest\", axis = 1).select_dtypes(include = ['object']).columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_cat_onehot = pd.get_dummies(dt_all_raw[cols_cat])\n",
    "dict_ohe = {x: \"Encode_ohe_\" + x for x in dt_cat_onehot.columns.values}\n",
    "dt_cat_onehot = dt_cat_onehot.rename(columns = dict_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 TargetMean Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# oof to encode cols_cat with TargetMean\n",
    "def getTargetMean(dt, dt_all, cols, k = 3):\n",
    "\n",
    "    # init dt_targetMean\n",
    "    dt_targetMean = pd.DataFrame()\n",
    "\n",
    "    for col in cols:\n",
    "    \n",
    "        # init dt_targetMean_oof\n",
    "        dt_targetMean_oof = pd.DataFrame()\n",
    "        \n",
    "        # X_targetMean_Kfold, y_targetMean_Kfold\n",
    "        X_targetMean_Kfold = dt[[col, \"y\"]]\n",
    "        y_targetMean_Kfold = dt[col].values\n",
    "\n",
    "        # oof cv\n",
    "        skf = StratifiedKFold(n_splits = k)\n",
    "        \n",
    "        \n",
    "        for i, (ind_in, ind_out) in enumerate(skf.split(X_targetMean_Kfold, y_targetMean_Kfold)):\n",
    "\n",
    "\n",
    "            # init dt_targetMean_oof\n",
    "            dt_targetMean_val = pd.DataFrame()\n",
    "\n",
    "            # X_in, X_out, y_in, y_out\n",
    "            X_in, X_out = X_targetMean_Kfold.iloc[ind_in], X_targetMean_Kfold.iloc[ind_out]\n",
    "            y_in, y_out = y_targetMean_Kfold[ind_in], y_targetMean_Kfold[ind_out]\n",
    "\n",
    "            # calc TargetMean\n",
    "            for val in set(X_in[col].values):\n",
    "                dt_targetMean_temp = pd.DataFrame({\"Value\": [val]\n",
    "                                                   , \"TargetMean_\" + str(i): [X_in.loc[X_in[col] == val].y.mean()]})\n",
    "                dt_targetMean_val = pd.concat([dt_targetMean_val, dt_targetMean_temp])\n",
    "\n",
    "            # merge with oof\n",
    "            if i == 0:\n",
    "                dt_targetMean_oof = pd.merge(X_targetMean_Kfold.drop(\"y\", axis = 1).drop_duplicates(), dt_targetMean_val\n",
    "                                             , how = \"left\", left_on = col, right_on = \"Value\")\n",
    "                dt_targetMean_oof = dt_targetMean_oof.drop(col, axis = 1)\n",
    "            else:\n",
    "                dt_targetMean_oof = pd.merge(dt_targetMean_oof, dt_targetMean_val\n",
    "                                             , how = \"left\", on = \"Value\")\n",
    "\n",
    "        # move Value to the first column\n",
    "        value = dt_targetMean_oof['Value']\n",
    "        dt_targetMean_oof.drop(labels = [\"Value\"], axis = 1,inplace = True)\n",
    "        dt_targetMean_oof.insert(0, 'Value', value)\n",
    "        # assign col\n",
    "        dt_targetMean_oof.insert(0, 'Col', col)\n",
    "        \n",
    "        # concat with col\n",
    "        dt_targetMean = pd.concat([dt_targetMean, dt_targetMean_oof])\n",
    "    \n",
    "    # mean of oof\n",
    "    dt_targetMean[\"TargetMean\"] = dt_targetMean.filter(regex = \"TargetMean\").mean(axis = 1)\n",
    "    # fill zero\n",
    "    dt_targetMean = dt_targetMean.fillna(0)\n",
    "    # remove oof cols\n",
    "    dt_targetMean = dt_targetMean[[\"Col\", \"Value\", \"TargetMean\"]]\n",
    "\n",
    "    # merge to original table\n",
    "    dt_cat_cols = dt_all[cols]\n",
    "    \n",
    "    for col in cols_cat:\n",
    "        dt_cat_cols = pd.merge(dt_cat_cols, dt_targetMean.loc[dt_targetMean[\"Col\"] == col]\n",
    "                           , how = \"left\", left_on = col, right_on = \"Value\")\n",
    "        dt_cat_cols = dt_cat_cols.drop([\"Value\", \"Col\"], axis = 1)\n",
    "        dt_cat_cols = dt_cat_cols.rename(columns = {\"TargetMean\": \"Encode_TargetMean_\" + col})\n",
    "    # fill zero\n",
    "    dt_cat_cols = dt_cat_cols.fillna(0)\n",
    "    \n",
    "    # only select targetMean cols\n",
    "    dt_cat_targetMean = dt_cat_cols.filter(regex = \"Encode_TargetMean_\")\n",
    "        \n",
    "    return dt_cat_targetMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "## remove the outlier and tm\n",
    "dt_cat_targetMean = getTargetMean(dt_all_raw.loc[(dt_all_raw[\"IsTrainTest\"] == \"train\") & (dt_all_raw[\"ID\"] != 1770)]\n",
    "                                  , dt_all_raw\n",
    "                                  , cols_cat\n",
    "                                  , 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Frequency Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFrequency(dt, cols):\n",
    "    # calc the cols freq\n",
    "    dt_cat_freq = pd.DataFrame()\n",
    "    for col in cols:\n",
    "        dt_col_freq_temp = pd.DataFrame({\"Col\": col\n",
    "                                         , \"Value\": dt[col].value_counts().index.values\n",
    "                                         , \"Freq\": dt[col].value_counts().values})\n",
    "\n",
    "        dt_cat_freq = pd.concat([dt_cat_freq, dt_col_freq_temp])\n",
    "\n",
    "    # merge to origin\n",
    "    dt_cat_cols = dt_all_raw[cols]\n",
    "    for col in cols:\n",
    "        dt_cat_cols = pd.merge(dt_cat_cols, dt_cat_freq.loc[dt_cat_freq[\"Col\"] == col]\n",
    "                           , how = \"left\", left_on = col, right_on = \"Value\")\n",
    "        dt_cat_cols = dt_cat_cols.drop([\"Value\", \"Col\"], axis = 1)\n",
    "        dt_cat_cols = dt_cat_cols.rename(columns = {\"Freq\": \"Encode_Freq_\" + col})\n",
    "\n",
    "    dt_cat_cols = dt_cat_cols.filter(regex = \"Encode_Freq_\")\n",
    "    \n",
    "    return dt_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_cat_freq = getFrequency(dt_all_raw, cols_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noahhhhhh/Env/deepNoah/lib/python3.5/site-packages/category_encoders/ordinal.py:178: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  X[col] = X[col].astype(int).reshape(-1, )\n",
      "/home/noahhhhhh/Env/deepNoah/lib/python3.5/site-packages/category_encoders/ordinal.py:167: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  X[switch.get('col')] = X[switch.get('col')].astype(int).reshape(-1, )\n"
     ]
    }
   ],
   "source": [
    "encode_binary = ce.BinaryEncoder(cols_cat)\n",
    "dt_cat_binary = encode_binary.fit_transform(dt_all_raw[cols_cat])\n",
    "dt_cat_binary.columns = \"Encode_Binary\" + dt_cat_binary.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4 Ordinal X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOrdinal(dt, col):\n",
    "    dt_ordinal = pd.DataFrame()\n",
    "    for val in set(dt[col].values):\n",
    "        dt_ordinal_temp = pd.DataFrame({\"Value\": [val]\n",
    "                                        , \"Encode_Ordinal_\" + col: dt.loc[dt[col] == val].y.mean()})\n",
    "        dt_ordinal = pd.concat([dt_ordinal, dt_ordinal_temp])\n",
    "\n",
    "    dt_cat_ordinal = pd.merge(dt[col].to_frame(), dt_ordinal\n",
    "                              , how = \"left\", left_on = col, right_on = \"Value\")\n",
    "    dt_cat_ordinal = dt_cat_ordinal.drop([col, \"Value\"], axis = 1)\n",
    "    return dt_cat_ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_cat_ordinal_X0 = getOrdinal(dt_all_raw, \"X0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.5 Combine all encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 545)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OHE\n",
    "dt_all_encoded = dt_all_raw.drop(cols_cat, axis = 1).join(dt_cat_onehot)\n",
    "dt_all_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 553)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TargetMean\n",
    "dt_all_encoded = dt_all_encoded.join(dt_cat_targetMean)\n",
    "dt_all_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 561)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency\n",
    "dt_all_encoded = dt_all_encoded.join(dt_cat_freq)\n",
    "dt_all_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 598)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary\n",
    "dt_all_encoded = dt_all_encoded.join(dt_cat_binary)\n",
    "dt_all_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ordinal X0\n",
    "dt_all_encoded = dt_all_encoded.join(dt_cat_ordinal_X0)\n",
    "dt_all_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.5 Save dt_all_raw, dt_all_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_all_encoded.to_csv(\"../../data/Mercedes_Benz_Greener_Manufacturing/data/dt_all_encoded.csv\", index = False)\n",
    "dt_all_raw.to_csv(\"../../data/Mercedes_Benz_Greener_Manufacturing/data/dt_all_raw.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 342)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read dt_all_raw\n",
    "dt_all_raw = pd.read_csv(\"../../data/Mercedes_Benz_Greener_Manufacturing/data/dt_all_raw.csv\")\n",
    "dt_all_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 599)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read dt_all_encoded\n",
    "dt_all_encoded = pd.read_csv(\"../../data/Mercedes_Benz_Greener_Manufacturing/data/dt_all_encoded.csv\")\n",
    "dt_all_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read cols_raw\n",
    "cols_raw = pickle.load(open(\"../../data/Mercedes_Benz_Greener_Manufacturing/data/cols_raw.pkl\", \"rb\"))\n",
    "len(cols_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Outlier marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOutlierMarker(dt, index_outlier = 883):\n",
    "    # outlier\n",
    "    dt_outlier = dt.loc[dt.index.values == index_outlier]\n",
    "    # calc int_outlierMarker\n",
    "    int_outlierMarker = np.zeros(dt.shape[0])\n",
    "    for col in dt_outlier.columns.values:\n",
    "        for val in dt_outlier[col].values:\n",
    "            int_outlierMarker = int_outlierMarker + (dt[col].values == val).astype(\"int64\")\n",
    "    \n",
    "    return int_outlierMarker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 OutlierMarker_Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_cat = dt_all_raw.drop([\"ID\", \"y\", \"IsTrainTest\"], axis = 1).select_dtypes(include = ['object']).columns.values\n",
    "cols_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_outlierMarker_cat = getOutlierMarker(dt_all_raw[cols_cat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 OutlierMarker_Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['X10', 'single_train_dup_train_X11', 'X12', 'X13', 'X14',\n",
       "       'dup_test_X15', 'X16', 'dup_train_X17', 'X18', 'X19'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_int = dt_all_raw.drop([\"ID\", \"y\", \"IsTrainTest\", \"IsDupRow_All\", \"IsDupRow_Cat\", \"IsDupRow_Int\"], axis = 1).select_dtypes(include = ['int64']).columns.values\n",
    "cols_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_outlierMarker_int = getOutlierMarker(dt_all_raw[cols_int])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 OutlierMarker_All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8', 'X10',\n",
       "       'single_train_dup_train_X11'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_all = dt_all_raw.drop([\"ID\", \"y\", \"IsTrainTest\", \"IsDupRow_All\", \"IsDupRow_Cat\", \"IsDupRow_Int\"], axis = 1).columns.values\n",
    "cols_all[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_outlierMarker_all = getOutlierMarker(dt_all_raw[cols_all])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 OutlierMarker_X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_X0 = \"X0\"\n",
    "int_outlierMarker_x0 = getOutlierMarker(dt_all_raw[cols_X0].to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_all_encoded.loc[:, \"FeatEng_OutlierMarker_Cat\"] = int_outlierMarker_cat\n",
    "dt_all_encoded.loc[:, \"FeatEng_OutlierMarker_Int\"] = int_outlierMarker_int\n",
    "dt_all_encoded.loc[:, \"FeatEng_OutlierMarker_All\"] = int_outlierMarker_all\n",
    "dt_all_encoded.loc[:, \"FeatEng_OutlierMarker_X0\"] = int_outlierMarker_x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>y</th>\n",
       "      <th>X10</th>\n",
       "      <th>single_train_dup_train_X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>dup_test_X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>dup_train_X17</th>\n",
       "      <th>...</th>\n",
       "      <th>Encode_BinaryX8_0</th>\n",
       "      <th>Encode_BinaryX8_1</th>\n",
       "      <th>Encode_BinaryX8_2</th>\n",
       "      <th>Encode_BinaryX8_3</th>\n",
       "      <th>Encode_BinaryX8_4</th>\n",
       "      <th>Encode_Ordinal_X0</th>\n",
       "      <th>FeatEng_OutlierMarker_Cat</th>\n",
       "      <th>FeatEng_OutlierMarker_Int</th>\n",
       "      <th>FeatEng_OutlierMarker_All</th>\n",
       "      <th>FeatEng_OutlierMarker_X0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>130.81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>47.583043</td>\n",
       "      <td>1.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>88.53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>47.583043</td>\n",
       "      <td>2.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>76.26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.638304</td>\n",
       "      <td>1.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>80.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40.638304</td>\n",
       "      <td>3.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>78.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.638304</td>\n",
       "      <td>2.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 603 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       y  X10  single_train_dup_train_X11  X12  X13  X14  dup_test_X15  \\\n",
       "0   0  130.81    0                           0    0    1    0             0   \n",
       "1   6   88.53    0                           0    0    0    0             0   \n",
       "2   7   76.26    0                           0    0    0    0             0   \n",
       "3   9   80.62    0                           0    0    0    0             0   \n",
       "4  13   78.02    0                           0    0    0    0             0   \n",
       "\n",
       "   X16  dup_train_X17            ...             Encode_BinaryX8_0  \\\n",
       "0    0              0            ...                             0   \n",
       "1    0              0            ...                             0   \n",
       "2    0              1            ...                             0   \n",
       "3    0              0            ...                             0   \n",
       "4    0              0            ...                             1   \n",
       "\n",
       "   Encode_BinaryX8_1  Encode_BinaryX8_2  Encode_BinaryX8_3  Encode_BinaryX8_4  \\\n",
       "0                  0                  0                  1                  0   \n",
       "1                  0                  0                  1                  0   \n",
       "2                  0                  1                  0                  1   \n",
       "3                  0                  1                  1                  0   \n",
       "4                  0                  0                  0                  0   \n",
       "\n",
       "   Encode_Ordinal_X0  FeatEng_OutlierMarker_Cat  FeatEng_OutlierMarker_Int  \\\n",
       "0          47.583043                        1.0                      271.0   \n",
       "1          47.583043                        2.0                      292.0   \n",
       "2          40.638304                        1.0                      266.0   \n",
       "3          40.638304                        3.0                      271.0   \n",
       "4          40.638304                        2.0                      275.0   \n",
       "\n",
       "   FeatEng_OutlierMarker_All  FeatEng_OutlierMarker_X0  \n",
       "0                      272.0                       0.0  \n",
       "1                      294.0                       0.0  \n",
       "2                      267.0                       0.0  \n",
       "3                      274.0                       0.0  \n",
       "4                      277.0                       0.0  \n",
       "\n",
       "[5 rows x 603 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_all_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Sum of binary cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Sum of all binary cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_binary_all = dt_all_encoded[cols_raw].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Sum of correlation-important binary cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corBin_Contin(dt, cols):\n",
    "    dt_binary_pointbiserialr = pd.DataFrame()\n",
    "    for col in cols:\n",
    "        cor_pb = stats.pointbiserialr(dt[col].values, dt.y.values)\n",
    "        dt_binary_pointbiserialr = pd.concat([dt_binary_pointbiserialr\n",
    "                                             , pd.DataFrame({\"Col\": col\n",
    "                                                            , \"Cor\": np.abs([cor_pb.correlation])\n",
    "                                                            , \"P\": [cor_pb.pvalue]})])\n",
    "    return dt_binary_pointbiserialr.sort_values(\"Cor\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_corBin_Cotin = corBin_Contin(dt_all_encoded, cols_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_binary_important_3 = dt_corBin_Cotin[\"Col\"].values[:3]\n",
    "cols_binary_important_5 = dt_corBin_Cotin[\"Col\"].values[:5]\n",
    "cols_binary_important_10 = dt_corBin_Cotin[\"Col\"].values[:10]\n",
    "cols_binary_important_20 = dt_corBin_Cotin[\"Col\"].values[:20]\n",
    "cols_binary_important_50 = dt_corBin_Cotin[\"Col\"].values[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_binary_important_3 = dt_all_encoded[cols_binary_important_3].sum(axis = 1)\n",
    "sum_binary_important_5 = dt_all_encoded[cols_binary_important_5].sum(axis = 1)\n",
    "sum_binary_important_10 = dt_all_encoded[cols_binary_important_10].sum(axis = 1)\n",
    "sum_binary_important_20 = dt_all_encoded[cols_binary_important_20].sum(axis = 1)\n",
    "sum_binary_important_50 = dt_all_encoded[cols_binary_important_50].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SumBin_All\n",
    "dt_all_encoded.loc[:, \"FeatEng_SumBin_All\"] = sum_binary_all\n",
    "# SumBin_Imp_X\n",
    "dt_all_encoded.loc[:, \"FeatEng_SumBin_Imp_3\"] = sum_binary_important_3\n",
    "dt_all_encoded.loc[:, \"FeatEng_SumBin_Imp_5\"] = sum_binary_important_5\n",
    "dt_all_encoded.loc[:, \"FeatEng_SumBin_Imp_10\"] = sum_binary_important_10\n",
    "dt_all_encoded.loc[:, \"FeatEng_SumBin_Imp_20\"] = sum_binary_important_20\n",
    "dt_all_encoded.loc[:, \"FeatEng_SumBin_Imp_50\"] = sum_binary_important_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 606)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# range\n",
    "pp_range = preprocessing.MinMaxScaler()\n",
    "mx_range = pp_range.fit_transform(dt_all_encoded.drop([\"ID\", \"y\", \"IsTrainTest\"], axis = 1))\n",
    "dt_all_eng = pd.DataFrame(mx_range, columns = dt_all_encoded.drop([\"ID\", \"y\", \"IsTrainTest\"], axis = 1).columns.values)\n",
    "dt_all_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featEng_dimRed(method, dt, n_component, cols, name_feature):\n",
    "    if method == \"PCA\":\n",
    "        dr = decomposition.PCA(n_components = n_component, random_state = 888)\n",
    "    elif method == \"ICA\":\n",
    "        dr = decomposition.FastICA(n_components = n_component, random_state = 888)\n",
    "    elif method == \"SVD\":\n",
    "        dr = decomposition.TruncatedSVD(n_components = n_component, random_state = 888)\n",
    "    elif method == \"FA\":\n",
    "        dr = decomposition.FactorAnalysis(n_components = n_component, random_state = 888)\n",
    "    mx = dr.fit_transform(dt[cols])\n",
    "    dt = dt.join(pd.DataFrame(mx, columns = [\"DR_\" + method + \"_\" + name_feature + \"_\" + str(i) for i in range(1, n_component + 1)]))\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.1.1 Raw binary cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 621)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_all_eng = featEng_dimRed(\"PCA\", dt_all_eng, 15, cols_raw, \"Raw_Bin\")\n",
    "dt_all_eng.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.1.2 Encoded cat cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 636)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_encode = dt_all_eng.filter(regex = \"Encode_\").columns.values\n",
    "dt_all_eng = featEng_dimRed(\"PCA\", dt_all_eng, 15, cols_encode, \"Encoded_Cat\")\n",
    "dt_all_eng.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.1.3 Feature engineed cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 639)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_featEng = dt_all_eng.filter(regex = \"FeatEng_\").columns.values\n",
    "dt_all_eng = featEng_dimRed(\"PCA\", dt_all_eng, 3, cols_featEng, \"FeatEng\")\n",
    "dt_all_eng.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.1.4 All cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 659)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_all = dt_all_eng.filter(regex = \"^((?!DR).)*$\").columns.values\n",
    "dt_all_eng = featEng_dimRed(\"PCA\", dt_all_eng, 20, cols_all, \"All\")\n",
    "dt_all_eng.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.1 Raw binary cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 674)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_all_eng = featEng_dimRed(\"ICA\", dt_all_eng, 15, cols_raw, \"Raw_Bin\")\n",
    "dt_all_eng.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.2 Encoded cat cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 689)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_encode = dt_all_eng.filter(regex = \"Encode_\").columns.values\n",
    "dt_all_eng = featEng_dimRed(\"ICA\", dt_all_eng, 15, cols_encode, \"Encoded_Cat\")\n",
    "dt_all_eng.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.3 Feature engineed cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 677)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_featEng = dt_all_eng.filter(regex = \"FeatEng_\").columns.values\n",
    "dt_all_eng = featEng_dimRed(\"ICA\", dt_all_eng, 3, cols_featEng, \"FeatEng\")\n",
    "dt_all_eng.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.4 All cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 697)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_all = dt_all_eng.filter(regex = \"^((?!DR).)*$\").columns.values\n",
    "dt_all_eng = featEng_dimRed(\"ICA\", dt_all_eng, 20, cols_all, \"All\")\n",
    "dt_all_eng.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3.1 Raw binary cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3.2 Encoded cat cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3.3 Feature engineed cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3.4 All cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Save dt_all_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_all_eng = dt_all_encoded\n",
    "dt_all_eng.to_csv(\"../../data/Mercedes_Benz_Greener_Manufacturing/data/dt_all_eng.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# r^2\n",
    "def r_2(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    # return a pair metric_name, result\n",
    "    # since preds are margin(before logistic transformation, cutoff at 0)\n",
    "    return 'score', r2_score(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 ExtraTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extratree\n",
    "params_extraTRee = {\n",
    "    \"n_jobs\": 7\n",
    "    , \"n_estimators\": 400\n",
    "    , \"max_depth\": 3\n",
    "    , \"min_samples_split\": 5\n",
    "    , \"random_state\": 888\n",
    "    , \"verbose\": 0\n",
    "}\n",
    "model_extra = ExtraTreesRegressor(**params_extraTRee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# params\n",
    "params_xgb = {\n",
    "    \"eta\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"objective\": 'reg:linear',\n",
    "    \"silent\": 0\n",
    "}\n",
    "num_boost_round = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_lr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ridge = linear_model.Ridge()\n",
    "params_ridge = {'alpha': [0,0.5,1,2,3,5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X, y, ID\n",
    "X_train_all = dt_all.loc[dt_all[\"IsTrainTest\"] == \"train\"].drop([\"ID\", \"y\", \"IsTrainTest\"], axis = 1)\n",
    "y_train_all = dt_all.loc[dt_all[\"IsTrainTest\"] == \"train\"].y.values\n",
    "ID_train_all = dt_all.loc[dt_all[\"IsTrainTest\"] == \"train\"].ID.values\n",
    "print(\"X_train_all:\", X_train_all.shape)\n",
    "\n",
    "X_test = dt_all.loc[dt_all[\"IsTrainTest\"] == \"test\"].drop([\"ID\", \"y\", \"IsTrainTest\"], axis = 1)\n",
    "y_test = dt_all.loc[dt_all[\"IsTrainTest\"] == \"test\"].y.values\n",
    "ID_test = dt_all.loc[dt_all[\"IsTrainTest\"] == \"test\"].ID.values\n",
    "print(\"X_test:\", X_test.shape)\n",
    "\n",
    "dtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import randint as sp_randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extratree\n",
    "params_extraTRee_randSearch = {\n",
    "    \"n_estimators\": sp_randint(200, 1000)\n",
    "    , \"max_depth\": sp_randint(3, 6)\n",
    "    , \"min_samples_split\": sp_randint(3, 20)\n",
    "    , \"criteriion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "model_extra_rs = ExtraTreesRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 888)\n",
    "rand_search = RandomizedSearchCV(model_extra_rs, params_extraTRee_randSearch\n",
    "                                 , scoring = scorer, cv = 3, verbose = 1, n_jobs = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scorer = make_scorer(r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_search.fit(X_train_all, y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_search.score(X_train_all, y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 888)\n",
    "skf.split(X_train_all, y_train_all)\n",
    "presds_y = cross_val_predict(model_extra, X_train_all, bin_y, cv = skf, n_jobs = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(presds_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "presds_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r2_score(bin_y, presds_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_rs_valid = 0\n",
    "preds_rs_test = []\n",
    "w_extra = .2\n",
    "w_xgb = .5\n",
    "w_ridge = .3\n",
    "for i in range(0, 10):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all\n",
    "                                                          , test_size = 0.2, random_state = i)\n",
    "    # extraTree\n",
    "    print(\"extraTree ...\")\n",
    "    model_extra.fit(X_train, y_train)\n",
    "    preds_extra_valid = model_extra.predict(X_valid)\n",
    "    score_extra = r2_score(y_valid, preds_extra_valid)\n",
    "    \n",
    "    # xgboost\n",
    "    print(\"xgboost ...\")\n",
    "    dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label = y_valid)\n",
    "    ls_watch =  [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    model_xgb = xgb.train(params_xgb, dtrain, evals = ls_watch\n",
    "                          , feval = r_2, maximize = True\n",
    "                          , num_boost_round = num_boost_round\n",
    "                          , early_stopping_rounds = 50, verbose_eval = 50)\n",
    "    preds_xgb_valid = model_xgb.predict(dvalid)\n",
    "    score_xgb = r2_score(y_valid, preds_xgb_valid)\n",
    "    \n",
    "    # ridge\n",
    "    print(\"ridge ...\")\n",
    "    model_ridge = model_ridge.fit(X_train, y_train)\n",
    "    preds_ridge_valid = model_ridge.predict(X_valid)\n",
    "    score_ridge = r2_score(y_valid, preds_ridge_valid)\n",
    "\n",
    "    # avg them\n",
    "    ls_preds = [preds_extra_valid * w_extra, preds_xgb_valid * w_xgb, preds_ridge_valid * w_ridge]\n",
    "    preds_rs_valid = [sum(e) for e in zip(*ls_preds)]\n",
    "    \n",
    "    score_rs = r2_score(y_valid, preds_rs_valid)\n",
    "    \n",
    "    score_rs_valid = score_rs_valid + score_rs / 10\n",
    "    \n",
    "    # test\n",
    "    preds_extra_test = model_extra.predict(X_test)\n",
    "    preds_xgb_test = model_xgb.predict(dtest)\n",
    "    preds_ridge_test = model_ridge.predict(X_test)\n",
    "    ls_preds_test = [preds_extra_test * w_extra, preds_xgb_test * w_xgb, preds_ridge_test * w_ridge]\n",
    "    preds_rs_test = [sum(e) for e in zip(*ls_preds_test)]\n",
    "\n",
    "    \n",
    "    print(\"i: {} - extraTree:{}; xgb:{}; ridge:{}; rs_all:{}\".format(i, round(score_extra, 5)\n",
    "                                                                     , round(score_xgb, 5)\n",
    "                                                                     , round(score_ridge, 5)\n",
    "                                                                     , round(score_rs, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_rs_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_rs_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Stratified Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_y = pd.qcut(y_train_all, 5, labels = [1, 2, 3, 4, 5]).astype(\"int64\")\n",
    "# stratified kfold\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_skf_valid = 0\n",
    "preds_skf_test = []\n",
    "w_extra = .2\n",
    "w_xgb = .5\n",
    "w_ridge = .3\n",
    "for i, (ind_train, ind_valid) in enumerate(skf.split(X_train_all, bin_y)):\n",
    "    # X, y\n",
    "    X_train, X_valid = X_train_all.iloc[ind_train], X_train_all.iloc[ind_valid]\n",
    "    y_train, y_valid = y_train_all[ind_train], y_train_all[ind_valid]\n",
    "    \n",
    "    # extraTree\n",
    "    print(\"extraTree ...\")\n",
    "    model_extra.fit(X_train, y_train)\n",
    "    preds_extra_valid = model_extra.predict(X_valid)\n",
    "    score_extra = r2_score(y_valid, preds_extra_valid)\n",
    "    \n",
    "    # xgboost\n",
    "    print(\"xgboost ...\")\n",
    "    dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label = y_valid)\n",
    "    ls_watch =  [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    model_xgb = xgb.train(params_xgb, dtrain, evals = ls_watch\n",
    "                          , feval = r_2, maximize = True\n",
    "                          , num_boost_round = num_boost_round\n",
    "                          , early_stopping_rounds = 50, verbose_eval = 50)\n",
    "    preds_xgb_valid = model_xgb.predict(dvalid)\n",
    "    score_xgb = r2_score(y_valid, preds_xgb_valid)\n",
    "    \n",
    "    # ridge\n",
    "    print(\"ridge ...\")\n",
    "    model_ridge = model_ridge.fit(X_train, y_train)\n",
    "    preds_ridge_valid = model_ridge.predict(X_valid)\n",
    "    score_ridge = r2_score(y_valid, preds_ridge_valid)\n",
    "\n",
    "    # avg them\n",
    "    ls_preds = ls_preds = [preds_extra_valid * w_extra, preds_xgb_valid * w_xgb, preds_ridge_valid * w_ridge]\n",
    "    preds_skf_valid = [sum(e) for e in zip(*ls_preds)]\n",
    "    \n",
    "    score_skf = r2_score(y_valid, preds_skf_valid)\n",
    "    \n",
    "    score_skf_valid = score_skf_valid + score_skf / 5\n",
    "    \n",
    "    # test\n",
    "    preds_extra_test = model_extra.predict(X_test)\n",
    "    preds_xgb_test = model_xgb.predict(dtest)\n",
    "    preds_ridge_test = model_ridge.predict(X_test)\n",
    "    ls_preds_test = [preds_extra_test * w_extra, preds_xgb_test * w_xgb, preds_ridge_test * w_ridge]\n",
    "    preds_skf_test = [sum(e) for e in zip(*ls_preds_test)]\n",
    "\n",
    "    \n",
    "    print(\"i: {} - extraTree:{}; xgb:{}; ridge:{}; rs_skf:{}\".format(i, round(score_extra, 5)\n",
    "                                                                     , round(score_xgb, 5)\n",
    "                                                                     , round(score_ridge, 5)\n",
    "                                                                     , round(score_skf, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_skf_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_skf_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wt_rs = .3\n",
    "wt_skf = .7\n",
    "ls_submit_preds_test = [np.array(preds_rs_test) * wt_rs, np.array(preds_skf_test) * wt_skf]\n",
    "preds_submit_test = [sum(e) for e in zip(*ls_submit_preds_test)]\n",
    "dt_submit = pd.DataFrame({\"ID\": ID_test, \"y\": preds_submit_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_submit.to_csv(\"../../data/Mercedes_Benz_Greener_Manufacturing/submission/4_initModel_ohe_tm_cvrs_cvskf_3_7_modextra_modxgb_modridge_2_5_3.csv\"\n",
    "                              , index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
